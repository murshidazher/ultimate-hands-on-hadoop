# Working with structured data

# Extends RDD to a "DataFrame" object

DataFrames:
	- Contain Row objects
	- Can run SQL queries
	- Has a schema (leading to more efficient storage)
	- Read and Write to JSON Hive, parquet
	- Communicates with JDBC/ODBC, Tableau

# Using SparkSQL in Python

from pyspark.sql import SQLContext, Row
hiveContext = HiveContext(sc)
inputData = spark.read.json(dataFile)
inputData.createOrReplaceTempView("myStructuredStuff")
myResultDataFrame = hiveContext.sql("""SELECT foo FROM bar ORDER BY foobar""")

# Other stuff you can do with dataframes
myResultDataFrame.show()
myResultDataFrame.select("someFieldName")
myResultDataFrame.filter(myResultDataFrame("someFieldName" > 200))
myResultDataFrame.groupBy(myResultDataFrame("someFieldName")).mean()
myResultDataFrame.rdd().map(mapperFunction)

# Datasets
# In Spark2.0, a DataFrame is really a DataSet of Row objects
# DataSets can wrap known, typed data too. But this is mostly transparent to you in Python, since Python is dynamically typed.
# So - dont' sweat this too much with Python. But the Spark 2.0 way is to use DataSets instead of DataFrames when you can.

# Shell access
# Spark SQL exposes a JDBC/OCBC server (if you built Spark with Hive support)
# Start it with sbin/start-thriftserver.sh
# Listens on port 10000 by default
# Connect using bin/beeline -u jdbc:hive2://localhost:10000
# Viola, you have a SQL shell to Spark SQL
# You can create new tables, or query existing ones that were cached using hiveCtx.cacheTable("tableName");

# User-defined functions (UDF's)
from pyspark.sql.types import IntegerType
hiveCtx.registerFunction("square", lambda x: x*x, IntegerType())
df = hiveCtx.sql("SELECT square('someNumericFiled') FROM tableName")



